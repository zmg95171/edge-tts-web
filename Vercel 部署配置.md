# Vercel éƒ¨ç½²é…ç½®

æœ¬æ–‡æ¡£åŒ…å«å°† Edge-TTS Web åº”ç”¨éƒ¨ç½²åˆ° Vercel å¹³å°çš„å®Œæ•´é…ç½®æ–‡ä»¶å’Œè¯´æ˜ã€‚

## ğŸ“‹ éƒ¨ç½²å‰ç½®æ¡ä»¶

### 1. è´¦æˆ·ä¸å¹³å°å‡†å¤‡
- âœ… **Vercel è´¦æˆ·**ï¼šæ³¨å†Œ Vercel è´¦æˆ·ï¼ˆ[vercel.com](https://vercel.com)ï¼‰
- âœ… **Git ä»“åº“**ï¼šå°†é¡¹ç›®ä»£ç æ‰˜ç®¡åˆ° GitHubã€GitLab æˆ– Bitbucket
- âœ… **ç‰ˆæœ¬æ§åˆ¶**ï¼šç¡®ä¿é¡¹ç›®å·²åˆå§‹åŒ– Git ä»“åº“å¹¶æäº¤ä»£ç 

### 2. API æœåŠ¡å‡†å¤‡
- âœ… **TTS API æœåŠ¡**ï¼š`https://tts.2068.online`ï¼ˆæ–‡æœ¬è½¬è¯­éŸ³ï¼‰
- âœ… **Whisper API æœåŠ¡**ï¼š`https://whisper.2068.online`ï¼ˆè¯­éŸ³è½¬æ–‡å­—ï¼‰
- âœ… **OpenAI API Key**ï¼ˆå¦‚éœ€ LLM åŠŸèƒ½ï¼‰ï¼šä» [platform.openai.com](https://platform.openai.com) è·å–
- âœ… **Anthropic API Key**ï¼ˆå¦‚éœ€ Claude åŠŸèƒ½ï¼‰ï¼šä» [console.anthropic.com](https://console.anthropic.com) è·å–

### 3. å¼€å‘ç¯å¢ƒå‡†å¤‡
- âœ… **Node.js ç¯å¢ƒ**ï¼šå®‰è£… Node.jsï¼ˆå»ºè®® v18 æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰
- âœ… **åŒ…ç®¡ç†å™¨**ï¼šnpmã€yarn æˆ– pnpm
- âœ… **Vercel CLI**ï¼šå®‰è£… Vercel å‘½ä»¤è¡Œå·¥å…·
  ```bash
  npm i -g vercel
  ```

### 4. é¡¹ç›®æ–‡ä»¶æ£€æŸ¥æ¸…å•

ç¡®ä¿é¡¹ç›®åŒ…å«ä»¥ä¸‹æ–‡ä»¶ï¼š

```bash
EGDE-TTS-WEB/
â”œâ”€â”€ vercel.json                    # Vercel é…ç½®æ–‡ä»¶ â­
â”œâ”€â”€ package.json                   # é¡¹ç›®ä¾èµ–é…ç½®
â”œâ”€â”€ tsconfig.json                  # TypeScript é…ç½®
â”œâ”€â”€ vite.config.ts                 # Vite æ„å»ºé…ç½®
â”œâ”€â”€ api/                           # API è·¯ç”±ç›®å½•
â”‚   â”œâ”€â”€ tts-edge.ts               # TTS API ä»£ç† â­
â”‚   â”œâ”€â”€ whisper-edge.ts           # Whisper API ä»£ç† â­
â”‚   â””â”€â”€ llm-edge.ts               # LLM API ä»£ç†ï¼ˆå¯é€‰ï¼‰
â”œâ”€â”€ services/                      # å‰ç«¯æœåŠ¡
â”‚   â”œâ”€â”€ ttsService.ts
â”‚   â”œâ”€â”€ whisperService.ts
â”‚   â””â”€â”€ llmService.tsï¼ˆå¯é€‰ï¼‰
â”œâ”€â”€ components/                     # React ç»„ä»¶
â”œâ”€â”€ App.tsx
â”œâ”€â”€ index.html
â””â”€â”€ index.tsx
```

### 5. å®‰å…¨æ€§å‡†å¤‡ âš ï¸

**å…³é”®æ­¥éª¤ - å¿…é¡»æ‰§è¡Œï¼š**

```bash
# 1. æ£€æŸ¥ .gitignore æ–‡ä»¶ï¼Œç¡®ä¿åŒ…å«ï¼š
.env.local
.env.*.local
node_modules

# 2. å¦‚æœä¹‹å‰æäº¤è¿‡ .env.localï¼Œç«‹å³ä» Git å†å²ä¸­åˆ é™¤ï¼š
git filter-branch --force --index-filter \
  'git rm --cached --ignore-unmatch .env.local' \
  --prune-empty --tag-name-filter cat -- --all

# 3. å¼ºåˆ¶æ¨é€æ¸…ç†åçš„å†å²
git push origin --force --all
git push origin --force --tags
```

### 6. ç¯å¢ƒå˜é‡é…ç½®å‡†å¤‡

åœ¨ Vercel æ§åˆ¶å°ä¸­å‡†å¤‡ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

| å˜é‡å | å€¼ | å¿…éœ€ | è¯´æ˜ |
|--------|-----|------|------|
| `TTS_API_URL` | `https://tts.2068.online` | âœ… | TTS API åœ°å€ |
| `WHISPER_API_URL` | `https://whisper.2068.online` | âœ… | Whisper API åœ°å€ |
| `OPENAI_API_KEY` | `sk-xxxxx` | ğŸ”¶ | OpenAI å¯†é’¥ï¼ˆå¯é€‰ï¼‰ |
| `ANTHROPIC_API_KEY` | `sk-ant-xxxxx` | ğŸ”¶ | Anthropic å¯†é’¥ï¼ˆå¯é€‰ï¼‰ |

### 7. ä»£ç é…ç½®æ£€æŸ¥

**æ£€æŸ¥ vercel.json é…ç½®ï¼š**
```json
{
  "version": 2,
  "builds": [
    { "src": "index.html", "use": "@vercel/static" },
    { "src": "api/**/*.ts", "use": "@vercel/node" }
  ],
  "rewrites": [
    { "source": "/api/(.*)", "destination": "/api/$1" }
  ],
  "headers": [
    {
      "source": "/api/(.*)",
      "headers": [
        { "key": "Access-Control-Allow-Origin", "value": "*" },
        { "key": "Access-Control-Allow-Methods", "value": "GET, POST, OPTIONS" },
        { "key": "Access-Control-Allow-Headers", "value": "X-Requested-With, Content-Type, Authorization" }
      ]
    }
  ]
}
```

### 8. ä¾èµ–å®‰è£…

```bash
# å®‰è£…é¡¹ç›®ä¾èµ–
npm install

# ç¡®ä¿æ— ç‰ˆæœ¬å†²çª
npm audit fix
```

### 9. æœ¬åœ°æµ‹è¯•

```bash
# æœ¬åœ°æµ‹è¯• API è·¯ç”±ï¼ˆå¦‚æœä½¿ç”¨ Next.jsï¼‰
npm run dev

# æµ‹è¯•æ„å»º
npm run build
```

### 10. Vercel CLI ç™»å½•

```bash
# ç™»å½• Vercel
vercel login
```

## ğŸ¯ éƒ¨ç½²å‰çš„æœ€ç»ˆæ£€æŸ¥æ¸…å•

- [ ] Git ä»“åº“å·²åˆå§‹åŒ–å¹¶æ¨é€åˆ°è¿œç¨‹
- [ ] `.gitignore` å·²æ­£ç¡®é…ç½®ï¼ˆç‰¹åˆ«æ˜¯ç¯å¢ƒå˜é‡æ–‡ä»¶ï¼‰
- [ ] æ•æ„Ÿä¿¡æ¯å·²ä» Git å†å²ä¸­æ¸…é™¤
- [ ] æ‰€æœ‰ API ç«¯ç‚¹æ–‡ä»¶å·²åˆ›å»ºï¼ˆ`api/` ç›®å½•ï¼‰
- [ ] `vercel.json` é…ç½®æ­£ç¡®
- [ ] æœ¬åœ°æµ‹è¯•é€šè¿‡
- [ ] å·²æ³¨å†Œ Vercel è´¦æˆ·
- [ ] å·²å®‰è£… Vercel CLI
- [ ] API å¯†é’¥å·²å‡†å¤‡å¥½ï¼ˆå¦‚éœ€è¦ LLM åŠŸèƒ½ï¼‰
- [ ] å¤–éƒ¨ API æœåŠ¡ï¼ˆTTSã€Whisperï¼‰å¯æ­£å¸¸è®¿é—®

## ğŸ“ æ¨èéƒ¨ç½²æµç¨‹

```bash
# 1. æœ€ç»ˆæäº¤ä»£ç 
git add .
git commit -m "å‡†å¤‡éƒ¨ç½²åˆ° Vercel"
git push origin main

# 2. ç™»å½• Vercel
vercel login

# 3. é¦–æ¬¡éƒ¨ç½²ï¼ˆé¢„è§ˆï¼‰
vercel

# 4. é…ç½®ç¯å¢ƒå˜é‡ï¼ˆé€šè¿‡ Vercel æ§åˆ¶å°æˆ– CLIï¼‰
vercel env add TTS_API_URL
vercel env add WHISPER_API_URL

# 5. æ­£å¼éƒ¨ç½²
vercel --prod
```

## 1. vercel.json é…ç½®

åˆ›å»º `vercel.json` æ–‡ä»¶å¹¶æ”¾ç½®åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ï¼š

```json
{
  "version": 2,
  "builds": [
    {
      "src": "index.html",
      "use": "@vercel/static"
    },
    {
      "src": "api/**/*.ts",
      "use": "@vercel/node"
    }
  ],
  "rewrites": [
    {
      "source": "/api/(.*)",
      "destination": "/api/$1"
    }
  ],
  "headers": [
    {
      "source": "/api/(.*)",
      "headers": [
        {
          "key": "Access-Control-Allow-Origin",
          "value": "*"
        },
        {
          "key": "Access-Control-Allow-Methods",
          "value": "GET, POST, OPTIONS"
        },
        {
          "key": "Access-Control-Allow-Headers",
          "value": "X-Requested-With, Content-Type, Authorization"
        }
      ]
    }
  ],
  "env": {
    "TTS_API_URL": "https://tts.2068.online",
    "WHISPER_API_URL": "https://whisper.2068.online"
  }
}
```

## 2. API è·¯ç”±æ–‡ä»¶

### 2.1 TTS API ä»£ç†

åˆ›å»º `api/tts-edge.ts` æ–‡ä»¶ï¼š

```typescript
// Vercel Edge Function for TTS API proxy
import { NextRequest, NextResponse } from 'next/server';

const TTS_API_URL = process.env.TTS_API_URL || 'https://tts.2068.online';

export const config = {
  runtime: 'edge',
};

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    
    // æ„å»ºè¯·æ±‚å‚æ•°
    const searchParams = new URLSearchParams({
      text: body.text || '',
      voice: body.voice || 'zh-CN-XiaoxiaoNeural',
      rate: body.rate?.toString() || '0',
      volume: body.volume?.toString() || '100',
      pitch: body.pitch?.toString() || '0',
    });

    // åˆ›å»ºè¯·æ±‚åˆ°å®é™…çš„ TTS æœåŠ¡
    const response = await fetch(`${TTS_API_URL}/api/tts?${searchParams}`, {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json',
      },
    });

    if (!response.ok) {
      throw new Error(`TTS API responded with status: ${response.status}`);
    }

    // è·å–éŸ³é¢‘æ•°æ®
    const audioData = await response.arrayBuffer();
    
    // è¿”å›éŸ³é¢‘æµ
    return new NextResponse(audioData, {
      status: 200,
      headers: {
        'Content-Type': 'audio/mpeg',
        'Cache-Control': 'no-cache',
      },
    });
  } catch (error) {
    console.error('TTS API Error:', error);
    return NextResponse.json(
      { error: 'Failed to process TTS request', message: error instanceof Error ? error.message : 'Unknown error' },
      { status: 500 }
    );
  }
}

// æ”¯æŒ OPTIONS è¯·æ±‚ä»¥å¤„ç† CORS
export async function OPTIONS() {
  return new Response(null, {
    status: 200,
    headers: {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
      'Access-Control-Allow-Headers': 'X-Requested-With, Content-Type, Authorization',
    },
  });
}
```

### 2.2 Whisper API ä»£ç†

åˆ›å»º `api/whisper-edge.ts` æ–‡ä»¶ï¼š

```typescript
// Vercel Edge Function for Whisper API proxy
import { NextRequest, NextResponse } from 'next/server';

const WHISPER_API_URL = process.env.WHISPER_API_URL || 'https://whisper.2068.online';

export const config = {
  runtime: 'edge',
};

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData();
    
    // åˆ›å»ºæ–°çš„ FormData å‘é€åˆ°å®é™…çš„ Whisper æœåŠ¡
    const newFormData = new FormData();
    
    // è½¬å‘æ–‡ä»¶å’Œå…¶ä»–å‚æ•°
    for (const [key, value] of formData.entries()) {
      if (key === 'audio' && value instanceof File) {
        // åˆ›å»ºæ–°çš„ File å¯¹è±¡
        const arrayBuffer = await value.arrayBuffer();
        const newFile = new File([arrayBuffer], value.name, {
          type: value.type,
          lastModified: value.lastModified,
        });
        newFormData.append(key, newFile);
      } else {
        newFormData.append(key, value);
      }
    }

    // å‘é€è¯·æ±‚åˆ°å®é™…çš„ Whisper æœåŠ¡
    const response = await fetch(`${WHISPER_API_URL}/api/transcribe`, {
      method: 'POST',
      body: newFormData,
    });

    if (!response.ok) {
      throw new Error(`Whisper API responded with status: ${response.status}`);
    }

    // è·å–å“åº”æ•°æ®
    const responseData = await response.json();
    
    // è¿”å›è½¬å½•ç»“æœ
    return NextResponse.json(responseData, {
      status: 200,
      headers: {
        'Cache-Control': 'no-cache',
      },
    });
  } catch (error) {
    console.error('Whisper API Error:', error);
    return NextResponse.json(
      { error: 'Failed to process Whisper request', message: error instanceof Error ? error.message : 'Unknown error' },
      { status: 500 }
    );
  }
}

// æ”¯æŒ OPTIONS è¯·æ±‚ä»¥å¤„ç† CORS
export async function OPTIONS() {
  return new Response(null, {
    status: 200,
    headers: {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
      'Access-Control-Allow-Headers': 'X-Requested-With, Content-Type, Authorization',
    },
  });
}
```

## 3. ç¯å¢ƒå˜é‡é…ç½®

åœ¨ Vercel æ§åˆ¶å°ä¸­è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

- `TTS_API_URL`: `https://tts.2068.online`
- `WHISPER_API_URL`: `https://whisper.2068.online`

æˆ–è€…åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env.local` æ–‡ä»¶ï¼ˆæœ¬åœ°å¼€å‘ä½¿ç”¨ï¼‰ï¼š

```
TTS_API_URL=https://tts.2068.online
WHISPER_API_URL=https://whisper.2068.online
```

## 4. å®¢æˆ·ç«¯ API æœåŠ¡ä¿®æ”¹

ä¿®æ”¹ `services/ttsService.ts` æ–‡ä»¶ï¼š

```typescript
// ä¿®æ”¹åçš„ TTS æœåŠ¡
export const synthesizeSpeech = async (text: string, options: TTSOptions = {}) => {
  try {
    const response = await fetch('/api/tts-edge', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        text,
        voice: options.voice || 'zh-CN-XiaoxiaoNeural',
        rate: options.rate || 0,
        volume: options.volume || 100,
        pitch: options.pitch || 0,
      }),
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const audioBlob = await response.blob();
    const audioUrl = URL.createObjectURL(audioBlob);
    return audioUrl;
  } catch (error) {
    console.error('Error synthesizing speech:', error);
    throw error;
  }
};
```

ä¿®æ”¹ `services/whisperService.ts` æ–‡ä»¶ï¼š

```typescript
// ä¿®æ”¹åçš„ Whisper æœåŠ¡
export const transcribeAudio = async (audioFile: File): Promise<TranscriptionResult> => {
  try {
    const formData = new FormData();
    formData.append('audio', audioFile);
    
    const response = await fetch('/api/whisper-edge', {
      method: 'POST',
      body: formData,
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    return await response.json();
  } catch (error) {
    console.error('Error transcribing audio:', error);
    throw error;
  }
};
```

## 5. éƒ¨ç½²æ­¥éª¤

1. å®‰è£… Vercel CLIï¼š
```bash
npm i -g vercel
```

2. ç™»å½• Vercelï¼š
```bash
vercel login
```

3. éƒ¨ç½²é¡¹ç›®ï¼š
```bash
vercel --prod
```

æˆ–è€…é€šè¿‡ Vercel ç½‘ç«™è¿æ¥ GitHub ä»“åº“è¿›è¡Œè‡ªåŠ¨éƒ¨ç½²ã€‚

## 6. æ³¨æ„äº‹é¡¹

1. **Edge Functions ä¼˜åŠ¿**ï¼š
   - æ›´å¿«çš„å†·å¯åŠ¨æ—¶é—´
   - æ›´ä½çš„å»¶è¿Ÿ
   - æ›´å¥½çš„å…¨çƒåˆ†å¸ƒ
   - æ²¡æœ‰æ ‡å‡†çš„ 10 ç§’è¶…æ—¶é™åˆ¶

2. **CORS é…ç½®**ï¼š
   - å·²åœ¨ vercel.json ä¸­é…ç½®äº† CORS å¤´
   - æ¯ä¸ª API å‡½æ•°éƒ½åŒ…å« OPTIONS å¤„ç†

3. **é”™è¯¯å¤„ç†**ï¼š
   - æ‰€æœ‰ API å‡½æ•°éƒ½åŒ…å«å®Œæ•´çš„é”™è¯¯å¤„ç†
   - é”™è¯¯ä¿¡æ¯ä¼šè®°å½•åœ¨ Vercel å‡½æ•°æ—¥å¿—ä¸­

4. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - ä½¿ç”¨æµå¼å“åº”å¤„ç†éŸ³é¢‘æ•°æ®
   - è®¾ç½®é€‚å½“çš„ç¼“å­˜å¤´

5. **æˆæœ¬è€ƒè™‘**ï¼š
   - Vercel Edge Functions æŒ‰ä½¿ç”¨é‡è®¡è´¹
   - å…è´¹å¥—é¤åŒ…å«ä¸€å®šé¢åº¦

## 7. æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **API è¯·æ±‚è¶…æ—¶**ï¼š
   - æ£€æŸ¥åŸå§‹ API æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ
   - ç¡®è®¤ Edge Function æ—¥å¿—ä¸­çš„é”™è¯¯ä¿¡æ¯

2. **CORS é”™è¯¯**ï¼š
   - ç¡®è®¤ vercel.json ä¸­çš„å¤´é…ç½®
   - æ£€æŸ¥æµè§ˆå™¨æ§åˆ¶å°çš„é¢„æ£€è¯·æ±‚

3. **éŸ³é¢‘æ— æ³•æ’­æ”¾**ï¼š
   - æ£€æŸ¥è¿”å›çš„ Content-Type æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤éŸ³é¢‘æ•°æ®å®Œæ•´æ€§

### ç›‘æ§ä¸æ—¥å¿—

- Vercel æ§åˆ¶å°æä¾›å‡½æ•°æ‰§è¡Œæ—¥å¿—
- å¯ä»¥æŸ¥çœ‹è¯·æ±‚/å“åº”è¯¦æƒ…
- ç›‘æ§å‡½æ•°æ‰§è¡Œæ—¶é—´å’Œé”™è¯¯ç‡

## 8. é«˜çº§é…ç½®

### è‡ªå®šä¹‰åŸŸå

1. åœ¨ Vercel æ§åˆ¶å°ä¸­æ·»åŠ è‡ªå®šä¹‰åŸŸå
2. é…ç½® DNS è®°å½•
3. è®¾ç½® SSL è¯ä¹¦ï¼ˆè‡ªåŠ¨å¤„ç†ï¼‰

### æ€§èƒ½ç›‘æ§

```typescript
// åœ¨ API å‡½æ•°ä¸­æ·»åŠ æ€§èƒ½ç›‘æ§
export async function POST(request: NextRequest) {
  const startTime = Date.now();
  
  try {
    // ... å‡½æ•°é€»è¾‘ ...
    
    const duration = Date.now() - startTime;
    console.log(`Function execution time: ${duration}ms`);
    
    return response;
  } catch (error) {
    const duration = Date.now() - startTime;
    console.error(`Function failed after ${duration}ms:`, error);
    
    return errorResponse;
  }
}
```

## 9. å¤‡ç”¨æ–¹æ¡ˆ

å¦‚æœ Edge Functions ä¸æ»¡è¶³éœ€æ±‚ï¼Œå¯ä»¥è€ƒè™‘ï¼š

1. **Vercel å¸¸è§„å‡½æ•°**ï¼š
   - æ›´é•¿çš„æ‰§è¡Œæ—¶é—´ï¼ˆä½†ä»æœ‰è¶…æ—¶é™åˆ¶ï¼‰
   - æ›´å¤§çš„å†…å­˜é™åˆ¶

2. **æ··åˆéƒ¨ç½²**ï¼š
   - å‰ç«¯éƒ¨ç½²åˆ° Vercel
   - API æœåŠ¡éƒ¨ç½²åˆ°å…¶ä»–å¹³å°

3. **ä½¿ç”¨ Cloudflare Workers**ï¼š
   - ç±»ä¼¼çš„è¾¹ç¼˜è®¡ç®—å¹³å°
   - ä¸åŒçš„å®šä»·æ¨¡å‹

---

æœ¬é…ç½®æä¾›äº†å®Œæ•´çš„ Vercel éƒ¨ç½²æ–¹æ¡ˆï¼Œé€šè¿‡ Edge Functions ä½œä¸º API ä»£ç†ï¼Œè§£å†³äº† CORS é—®é¢˜ï¼Œå¹¶æä¾›äº†æ›´å¥½çš„æ€§èƒ½å’Œå¯é æ€§ã€‚

## 10. LLM API é›†æˆä¸å®‰å…¨é…ç½®

### 10.1 å®‰å…¨é—®é¢˜ä¸ç«‹å³ä¿®å¤

**é‡è¦å®‰å…¨è­¦å‘Š**ï¼šå¦‚æœé¡¹ç›®ä¸­åŒ…å« `.env.local` æ–‡ä»¶å¹¶ä¸”å·²ç»æäº¤åˆ° Gitï¼Œè¯·ç«‹å³æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ä¿®å¤ï¼š

1. ä» Git å†å²ä¸­å®Œå…¨åˆ é™¤æ•æ„Ÿä¿¡æ¯ï¼š
```bash
# ç«‹å³æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œåˆ é™¤å†å²ä¸­çš„æ•æ„Ÿä¿¡æ¯
git filter-branch --force --index-filter 'git rm --cached --ignore-unmatch .env.local' --prune-empty --tag-name-filter cat -- --all
```

2. æ›´æ–° `.gitignore` æ–‡ä»¶ï¼Œç¡®ä¿ `.env.local` æ°¸è¿œä¸ä¼šè¢«æäº¤ï¼š
```gitignore
# ç¯å¢ƒå˜é‡æ–‡ä»¶
.env.local
.env.*.local
```

3. å¼ºåˆ¶æ¨é€åˆ°è¿œç¨‹ä»“åº“ï¼š
```bash
git push origin --force --all
git push origin --force --tags
```

### 10.2 LLM API ä»£ç†å®ç°

åˆ›å»º `api/llm-edge.ts` æ–‡ä»¶ï¼Œä½œä¸ºå®‰å…¨çš„ LLM API ä»£ç†ï¼š

```typescript
// Vercel Edge Function for LLM API proxy
import { NextRequest, NextResponse } from 'next/server';

// è·å–ç¯å¢ƒå˜é‡ä¸­çš„ API å¯†é’¥
const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const ANTHROPIC_API_KEY = process.env.ANTHROPIC_API_KEY;

// è¯·æ±‚é™åˆ¶é…ç½®
const RATE_LIMIT_REQUESTS = 20; // æ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°
const MAX_TOKENS_PER_REQUEST = 2000; // å•æ¬¡è¯·æ±‚æœ€å¤§ä»¤ç‰Œæ•°
const MAX_CONTENT_LENGTH = 10000; // æœ€å¤§å†…å®¹é•¿åº¦

// ç®€å•çš„å†…å­˜å­˜å‚¨ç”¨äºé€Ÿç‡é™åˆ¶ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨ Redis æˆ–å…¶ä»–æŒä¹…åŒ–å­˜å‚¨ï¼‰
const requestCounts = new Map<string, { count: number; resetTime: number }>();

// æ¸…ç†è¿‡æœŸçš„é€Ÿç‡é™åˆ¶è®°å½•
function cleanExpiredEntries() {
  const now = Date.now();
  for (const [ip, data] of requestCounts.entries()) {
    if (now > data.resetTime) {
      requestCounts.delete(ip);
    }
  }
}

// æ£€æŸ¥é€Ÿç‡é™åˆ¶
function checkRateLimit(ip: string): boolean {
  cleanExpiredEntries();
  const now = Date.now();
  const windowStart = now - 60000; // 1åˆ†é’Ÿçª—å£
  
  if (!requestCounts.has(ip)) {
    requestCounts.set(ip, { count: 1, resetTime: windowStart + 60000 });
    return true;
  }
  
  const data = requestCounts.get(ip)!;
  
  if (now > data.resetTime) {
    data.count = 1;
    data.resetTime = now + 60000;
    return true;
  }
  
  if (data.count >= RATE_LIMIT_REQUESTS) {
    return false;
  }
  
  data.count++;
  return true;
}

// è·å–å®¢æˆ·ç«¯ IP
function getClientIP(request: NextRequest): string {
  return request.headers.get('x-forwarded-for') || 
         request.headers.get('x-real-ip') || 
         'unknown';
}

// éªŒè¯è¯·æ±‚å†…å®¹
function validateRequest(body: any): { isValid: boolean; error?: string } {
  if (!body.messages || !Array.isArray(body.messages)) {
    return { isValid: false, error: 'Invalid messages format' };
  }
  
  if (body.messages.length === 0) {
    return { isValid: false, error: 'Messages array cannot be empty' };
  }
  
  if (body.messages.length > 10) {
    return { isValid: false, error: 'Too many messages in request' };
  }
  
  // æ£€æŸ¥æ¯ä¸ªæ¶ˆæ¯çš„å†…å®¹
  for (const message of body.messages) {
    if (!message.role || !message.content) {
      return { isValid: false, error: 'Each message must have role and content' };
    }
    
    if (!['system', 'user', 'assistant'].includes(message.role)) {
      return { isValid: false, error: 'Invalid message role' };
    }
    
    if (typeof message.content !== 'string' || message.content.length > MAX_CONTENT_LENGTH) {
      return { isValid: false, error: `Message content too long (max ${MAX_CONTENT_LENGTH} characters)` };
    }
  }
  
  // æ£€æŸ¥ä»¤ç‰Œé™åˆ¶
  if (body.max_tokens && (typeof body.max_tokens !== 'number' || body.max_tokens > MAX_TOKENS_PER_REQUEST)) {
    return { isValid: false, error: `Invalid max_tokens (max ${MAX_TOKENS_PER_REQUEST})` };
  }
  
  return { isValid: true };
}

export const config = {
  runtime: 'edge',
};

export async function POST(request: NextRequest) {
  // è·å–å®¢æˆ·ç«¯ IP è¿›è¡Œé€Ÿç‡é™åˆ¶
  const clientIP = getClientIP(request);
  
  // æ£€æŸ¥é€Ÿç‡é™åˆ¶
  if (!checkRateLimit(clientIP)) {
    return NextResponse.json(
      { error: 'Rate limit exceeded. Please try again later.' },
      { status: 429 }
    );
  }
  
  try {
    // è§£æè¯·æ±‚ä½“
    const body = await request.json();
    
    // éªŒè¯è¯·æ±‚å†…å®¹
    const validation = validateRequest(body);
    if (!validation.isValid) {
      return NextResponse.json(
        { error: validation.error },
        { status: 400 }
      );
    }
    
    // ç¡®å®š API æä¾›å•†
    const provider = body.provider || 'openai';
    
    let apiUrl: string;
    let apiKey: string;
    let requestBody: any;
    
    if (provider === 'openai') {
      if (!OPENAI_API_KEY) {
        return NextResponse.json(
          { error: 'OpenAI API key not configured' },
          { status: 500 }
        );
      }
      
      apiUrl = 'https://api.openai.com/v1/chat/completions';
      apiKey = OPENAI_API_KEY;
      
      // æ„å»º OpenAI è¯·æ±‚ä½“
      requestBody = {
        model: body.model || 'gpt-3.5-turbo',
        messages: body.messages,
        max_tokens: body.max_tokens || 500,
        temperature: body.temperature || 0.7,
        stream: false, // æš‚ä¸æ”¯æŒæµå¼å“åº”
      };
    } else if (provider === 'anthropic') {
      if (!ANTHROPIC_API_KEY) {
        return NextResponse.json(
          { error: 'Anthropic API key not configured' },
          { status: 500 }
        );
      }
      
      apiUrl = 'https://api.anthropic.com/v1/messages';
      apiKey = ANTHROPIC_API_KEY;
      
      // æ„å»º Anthropic è¯·æ±‚ä½“
      requestBody = {
        model: body.model || 'claude-3-haiku-20240307',
        max_tokens: body.max_tokens || 500,
        messages: body.messages,
        temperature: body.temperature || 0.7,
      };
    } else {
      return NextResponse.json(
        { error: 'Unsupported provider' },
        { status: 400 }
      );
    }
    
    // å‘é€è¯·æ±‚åˆ° LLM API
    const response = await fetch(apiUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`,
        // Anthropic éœ€è¦ç‰¹æ®Šçš„ API ç‰ˆæœ¬å¤´
        ...(provider === 'anthropic' && { 'anthropic-version': '2023-06-01' }),
      },
      body: JSON.stringify(requestBody),
    });
    
    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      return NextResponse.json(
        { 
          error: 'LLM API request failed',
          status: response.status,
          details: errorData
        },
        { status: response.status }
      );
    }
    
    // è·å–å“åº”æ•°æ®
    const responseData = await response.json();
    
    // è®°å½•ä½¿ç”¨æƒ…å†µï¼ˆå¯é€‰ï¼‰
    console.log(`LLM API used: provider=${provider}, tokens=${responseData.usage?.total_tokens || 'unknown'}`);
    
    // è¿”å›å“åº”
    return NextResponse.json(responseData, {
      status: 200,
      headers: {
        'Cache-Control': 'no-cache',
      },
    });
  } catch (error) {
    console.error('LLM API Error:', error);
    return NextResponse.json(
      { error: 'Failed to process LLM request', message: error instanceof Error ? error.message : 'Unknown error' },
      { status: 500 }
    );
  }
}

// æ”¯æŒ OPTIONS è¯·æ±‚ä»¥å¤„ç† CORS
export async function OPTIONS() {
  return new Response(null, {
    status: 200,
    headers: {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
      'Access-Control-Allow-Headers': 'X-Requested-With, Content-Type, Authorization',
    },
  });
}
```

### 10.3 å‰ç«¯ LLM æœåŠ¡å®ç°

åˆ›å»º `services/llmService.ts` æ–‡ä»¶ï¼š

```typescript
// LLM æœåŠ¡æ¥å£å®šä¹‰
export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMOptions {
  provider?: 'openai' | 'anthropic';
  model?: string;
  maxTokens?: number;
  temperature?: number;
}

export interface LLMResponse {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: LLMMessage;
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

// LLM API è°ƒç”¨å‡½æ•°
export const callLLM = async (
  messages: LLMMessage[], 
  options: LLMOptions = {}
): Promise<LLMResponse> => {
  try {
    const response = await fetch('/api/llm-edge', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        messages,
        provider: options.provider || 'openai',
        model: options.model,
        max_tokens: options.maxTokens || 500,
        temperature: options.temperature || 0.7,
      }),
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.error || `HTTP error! status: ${response.status}`);
    }

    return await response.json();
  } catch (error) {
    console.error('Error calling LLM API:', error);
    throw error;
  }
};

// é¢„è®¾çš„å¸¸ç”¨ç³»ç»Ÿæ¶ˆæ¯
export const SYSTEM_PROMPTS = {
  TRANSLATOR: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ï¼Œè¯·å°†ç”¨æˆ·æä¾›çš„å†…å®¹å‡†ç¡®ç¿»è¯‘æˆæŒ‡å®šè¯­è¨€ã€‚',
  SUMMARIZER: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ€»ç»“åŠ©æ‰‹ï¼Œè¯·ç®€æ´å‡†ç¡®åœ°æ€»ç»“ç”¨æˆ·æä¾›çš„å†…å®¹ã€‚',
  EXPLAINER: 'ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†ä¸°å¯Œçš„è§£é‡ŠåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€å•æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šç”¨æˆ·çš„é—®é¢˜ã€‚',
  CREATOR: 'ä½ æ˜¯ä¸€ä¸ªåˆ›æ„å†™ä½œåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„æç¤ºåˆ›ä½œæœ‰è¶£çš„å†…å®¹ã€‚',
};

// ä¾¿æ·çš„ç¿»è¯‘å‡½æ•°
export const translateText = async (
  text: string, 
  targetLanguage: string,
  sourceLanguage: string = 'auto'
): Promise<string> => {
  const messages: LLMMessage[] = [
    {
      role: 'system',
      content: `${SYSTEM_PROMPTS.TRANSLATOR} è¯·å°†${sourceLanguage === 'auto' ? 'è‡ªåŠ¨æ£€æµ‹è¯­è¨€' : sourceLanguage}ç¿»è¯‘æˆ${targetLanguage}ï¼Œåªè¿”å›ç¿»è¯‘ç»“æœï¼Œä¸éœ€è¦è§£é‡Šã€‚`,
    },
    {
      role: 'user',
      content: text,
    },
  ];

  const response = await callLLM(messages, {
    maxTokens: 1000,
    temperature: 0.3, // è¾ƒä½æ¸©åº¦ä»¥è·å¾—æ›´å‡†ç¡®çš„ç¿»è¯‘
  });

  return response.choices[0]?.message?.content || 'ç¿»è¯‘å¤±è´¥';
};

// ä¾¿æ·çš„æ€»ç»“å‡½æ•°
export const summarizeText = async (text: string): Promise<string> => {
  const messages: LLMMessage[] = [
    {
      role: 'system',
      content: SYSTEM_PROMPTS.SUMMARIZER,
    },
    {
      role: 'user',
      content: `è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š\n\n${text}`,
    },
  ];

  const response = await callLLM(messages, {
    maxTokens: 500,
    temperature: 0.5,
  });

  return response.choices[0]?.message?.content || 'æ€»ç»“å¤±è´¥';
};
```

### 10.4 ç¯å¢ƒå˜é‡ä¸ API å¯†é’¥ç®¡ç†

åœ¨ Vercel æ§åˆ¶å°ä¸­å®‰å…¨åœ°è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

```bash
# OpenAI API å¯†é’¥
OPENAI_API_KEY=sk-your-openai-api-key-here

# Anthropic (Claude) API å¯†é’¥
ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here
```

**é‡è¦å®‰å…¨æç¤º**ï¼š
1. ç»ä¸åœ¨ä»£ç ä¸­ç¡¬ç¼–ç  API å¯†é’¥
2. ç»ä¸å°† `.env.local` æ–‡ä»¶æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿ
3. ä½¿ç”¨ Vercel çš„åŠ å¯†ç¯å¢ƒå˜é‡åŠŸèƒ½å­˜å‚¨æ•æ„Ÿä¿¡æ¯
4. å®šæœŸè½®æ¢ API å¯†é’¥

### 10.5 æ›´æ–° vercel.json é…ç½®

```json
{
  "version": 2,
  "builds": [
    {
      "src": "index.html",
      "use": "@vercel/static"
    },
    {
      "src": "api/**/*.ts",
      "use": "@vercel/node"
    }
  ],
  "rewrites": [
    {
      "source": "/api/(.*)",
      "destination": "/api/$1"
    }
  ],
  "headers": [
    {
      "source": "/api/(.*)",
      "headers": [
        {
          "key": "Access-Control-Allow-Origin",
          "value": "*"
        },
        {
          "key": "Access-Control-Allow-Methods",
          "value": "GET, POST, OPTIONS"
        },
        {
          "key": "Access-Control-Allow-Headers",
          "value": "X-Requested-With, Content-Type, Authorization"
        }
      ]
    }
  ],
  "env": {
    "TTS_API_URL": "https://tts.2068.online",
    "WHISPER_API_URL": "https://whisper.2068.online"
  },
  "functions": {
    "api/llm-edge.ts": {
      "maxDuration": 30
    }
  }
}
```

### 10.6 LLM API ä½¿ç”¨ç¤ºä¾‹

åˆ›å»º `components/LLMExample.tsx` ç»„ä»¶ï¼š

```typescript
import React, { useState } from 'react';
import { callLLM, translateText, summarizeText, LLMMessage } from '../services/llmService';

const LLMExample: React.FC = () => {
  const [inputText, setInputText] = useState('');
  const [outputText, setOutputText] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [mode, setMode] = useState<'chat' | 'translate' | 'summarize'>('chat');
  const [targetLanguage, setTargetLanguage] = useState('è‹±è¯­');

  const handleChat = async () => {
    if (!inputText.trim()) return;
    
    setIsLoading(true);
    try {
      const messages: LLMMessage[] = [
        {
          role: 'system',
          content: 'ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ï¼Œè¯·ç®€æ´å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚',
        },
        {
          role: 'user',
          content: inputText,
        },
      ];
      
      const response = await callLLM(messages);
      setOutputText(response.choices[0]?.message?.content || 'æ— æ³•è·å–å›å¤');
    } catch (error) {
      setOutputText(`é”™è¯¯: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`);
    } finally {
      setIsLoading(false);
    }
  };

  const handleTranslate = async () => {
    if (!inputText.trim()) return;
    
    setIsLoading(true);
    try {
      const translation = await translateText(inputText, targetLanguage);
      setOutputText(translation);
    } catch (error) {
      setOutputText(`ç¿»è¯‘é”™è¯¯: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`);
    } finally {
      setIsLoading(false);
    }
  };

  const handleSummarize = async () => {
    if (!inputText.trim()) return;
    
    setIsLoading(true);
    try {
      const summary = await summarizeText(inputText);
      setOutputText(summary);
    } catch (error) {
      setOutputText(`æ€»ç»“é”™è¯¯: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`);
    } finally {
      setIsLoading(false);
    }
  };

  const handleSubmit = () => {
    switch (mode) {
      case 'chat':
        handleChat();
        break;
      case 'translate':
        handleTranslate();
        break;
      case 'summarize':
        handleSummarize();
        break;
    }
  };

  return (
    <div className="llm-example">
      <h2>LLM åŠŸèƒ½ç¤ºä¾‹</h2>
      
      <div className="mode-selector">
        <button 
          className={mode === 'chat' ? 'active' : ''} 
          onClick={() => setMode('chat')}
        >
          å¯¹è¯
        </button>
        <button 
          className={mode === 'translate' ? 'active' : ''} 
          onClick={() => setMode('translate')}
        >
          ç¿»è¯‘
        </button>
        <button 
          className={mode === 'summarize' ? 'active' : ''} 
          onClick={() => setMode('summarize')}
        >
          æ€»ç»“
        </button>
      </div>
      
      {mode === 'translate' && (
        <div className="translation-options">
          <label>
            ç›®æ ‡è¯­è¨€:
            <select 
              value={targetLanguage} 
              onChange={(e) => setTargetLanguage(e.target.value)}
            >
              <option value="è‹±è¯­">è‹±è¯­</option>
              <option value="æ—¥è¯­">æ—¥è¯­</option>
              <option value="éŸ©è¯­">éŸ©è¯­</option>
              <option value="æ³•è¯­">æ³•è¯­</option>
              <option value="å¾·è¯­">å¾·è¯­</option>
              <option value="è¥¿ç­ç‰™è¯­">è¥¿ç­ç‰™è¯­</option>
            </select>
          </label>
        </div>
      )}
      
      <div className="input-section">
        <textarea
          value={inputText}
          onChange={(e) => setInputText(e.target.value)}
          placeholder={mode === 'chat' ? 'è¾“å…¥ä½ çš„é—®é¢˜...' : 
                   mode === 'translate' ? 'è¾“å…¥è¦ç¿»è¯‘çš„æ–‡æœ¬...' : 
                   'è¾“å…¥è¦æ€»ç»“çš„æ–‡æœ¬...'}
          rows={5}
        />
        <button 
          onClick={handleSubmit} 
          disabled={isLoading || !inputText.trim()}
        >
          {isLoading ? 'å¤„ç†ä¸­...' : 'æäº¤'}
        </button>
      </div>
      
      {outputText && (
        <div className="output-section">
          <h3>ç»“æœ:</h3>
          <p>{outputText}</p>
        </div>
      )}
      
      <div className="usage-info">
        <p>
          <strong>æ³¨æ„:</strong> LLM API ä½¿ç”¨æœ‰é…é¢é™åˆ¶ï¼Œè¯·åˆç†ä½¿ç”¨ã€‚ 
          æ¯æ¬¡è¯·æ±‚éƒ½ä¼šæ¶ˆè€—ä¸€å®šæ•°é‡çš„ä»¤ç‰Œï¼Œå…·ä½“å–å†³äºè¾“å…¥å’Œè¾“å‡ºçš„é•¿åº¦ã€‚
        </p>
      </div>
    </div>
  );
};

export default LLMExample;
```

### 10.7 API å¯†é’¥è½®æ¢ç­–ç•¥

å®ç°å®‰å…¨çš„ API å¯†é’¥è½®æ¢æœºåˆ¶ï¼š

1. **å®šæœŸè½®æ¢**ï¼š
   - è®¾ç½®æ¯æœˆæˆ–æ¯å­£åº¦çš„ API å¯†é’¥è½®æ¢æé†’
   - ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬æ‰¹é‡æ›´æ–°ç¯å¢ƒå˜é‡

2. **ç‰ˆæœ¬æ§åˆ¶**ï¼š
   - åœ¨ç¯å¢ƒå˜é‡åç§°ä¸­åŒ…å«ç‰ˆæœ¬å·
   - ç¤ºä¾‹ï¼š`OPENAI_API_KEY_V2`

3. **å›é€€æœºåˆ¶**ï¼š
   - ä¿ç•™æ—§å¯†é’¥ä¸€æ®µæ—¶é—´ä½œä¸ºå›é€€
   - é€æ­¥è¿ç§»æµé‡åˆ°æ–°å¯†é’¥

```typescript
// åœ¨ api/llm-edge.ts ä¸­æ·»åŠ å¯†é’¥è½®æ¢é€»è¾‘
const getApiKey = (provider: string): string => {
  if (provider === 'openai') {
    // å°è¯•æ–°å¯†é’¥ï¼Œå¦‚æœæ— æ•ˆåˆ™ä½¿ç”¨æ—§å¯†é’¥
    return process.env.OPENAI_API_KEY_V2 || process.env.OPENAI_API_KEY || '';
  } else if (provider === 'anthropic') {
    return process.env.ANTHROPIC_API_KEY_V2 || process.env.ANTHROPIC_API_KEY || '';
  }
  return '';
};
```

### 10.8 ä½¿ç”¨ç›‘æ§ä¸æˆæœ¬æ§åˆ¶

å®ç°ä½¿ç”¨ç›‘æ§å’Œæˆæœ¬æ§åˆ¶æœºåˆ¶ï¼š

1. **ä½¿ç”¨é‡ç›‘æ§**ï¼š
   ```typescript
   // åœ¨ api/llm-edge.ts ä¸­æ·»åŠ ä½¿ç”¨é‡è®°å½•
   interface UsageRecord {
     timestamp: number;
     provider: string;
     model: string;
     tokens: number;
     cost: number;
   }
   
   // ç®€å•çš„å†…å­˜å­˜å‚¨ï¼ˆç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨æ•°æ®åº“ï¼‰
   const usageRecords: UsageRecord[] = [];
   
   // è®°å½•ä½¿ç”¨æƒ…å†µ
   function recordUsage(
     provider: string, 
     model: string, 
     tokens: number
   ) {
     const cost = calculateCost(provider, model, tokens);
     usageRecords.push({
       timestamp: Date.now(),
       provider,
       model,
       tokens,
       cost
     });
   }
   
   // è®¡ç®—è´¹ç”¨ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼‰
   function calculateCost(provider: string, model: string, tokens: number): number {
     const pricing: Record<string, number> = {
       'openai-gpt-3.5-turbo': 0.002 / 1000, // æ¯ 1000 ä»¤ç‰Œ $0.002
       'openai-gpt-4': 0.03 / 1000,        // æ¯ 1000 ä»¤ç‰Œ $0.03
       'anthropic-claude-3-haiku': 0.00025 / 1000, // æ¯ 1000 ä»¤ç‰Œ $0.00025
     };
     
     const key = `${provider}-${model}`;
     return (pricing[key] || 0) * tokens;
   }
   ```

2. **æˆæœ¬æ§åˆ¶**ï¼š
   ```typescript
   // åœ¨ api/llm-edge.ts ä¸­æ·»åŠ æˆæœ¬æ§åˆ¶
   const MONTHLY_COST_LIMIT = 50; // æ¯æœˆ $50 é™åˆ¶
   
   function checkMonthlyLimit(): boolean {
     const now = Date.now();
     const monthStart = new Date(now.getFullYear(), now.getMonth(), 1).getTime();
     
     // è®¡ç®—æœ¬æœˆä½¿ç”¨é‡
     const monthlyUsage = usageRecords
       .filter(record => record.timestamp >= monthStart)
       .reduce((total, record) => total + record.cost, 0);
     
     return monthlyUsage < MONTHLY_COST_LIMIT;
   }
   ```

3. **ç®¡ç†é¢æ¿**ï¼š
   åˆ›å»ºä¸€ä¸ªç®€å•çš„ä½¿ç”¨æƒ…å†µç›‘æ§é¢æ¿ï¼š
   ```typescript
   // components/UsageMonitor.tsx
   import React, { useState, useEffect } from 'react';
   
   interface UsageData {
     dailyUsage: number;
     monthlyUsage: number;
     costLimit: number;
     remainingBudget: number;
   }
   
   const UsageMonitor: React.FC = () => {
     const [usageData, setUsageData] = useState<UsageData | null>(null);
     const [isLoading, setIsLoading] = useState(true);
   
     useEffect(() => {
       // è·å–ä½¿ç”¨æƒ…å†µæ•°æ®
       const fetchUsageData = async () => {
         try {
           // åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨ä¸“é—¨çš„ API ç«¯ç‚¹
           const response = await fetch('/api/usage-stats');
           const data = await response.json();
           setUsageData(data);
         } catch (error) {
           console.error('è·å–ä½¿ç”¨æƒ…å†µå¤±è´¥:', error);
         } finally {
           setIsLoading(false);
         }
       };
   
       fetchUsageData();
     }, []);
   
     if (isLoading) {
       return <div>åŠ è½½ä½¿ç”¨æƒ…å†µä¸­...</div>;
     }
   
     if (!usageData) {
       return <div>æ— æ³•è·å–ä½¿ç”¨æƒ…å†µ</div>;
     }
   
     const usagePercentage = (usageData.monthlyUsage / usageData.costLimit) * 100;
   
     return (
       <div className="usage-monitor">
         <h3>API ä½¿ç”¨æƒ…å†µ</h3>
         
         <div className="usage-stats">
           <div className="stat-item">
             <span className="label">ä»Šæ—¥ä½¿ç”¨:</span>
             <span className="value">{usageData.dailyUsage.toFixed(4)} ç¾å…ƒ</span>
           </div>
           
           <div className="stat-item">
             <span className="label">æœ¬æœˆä½¿ç”¨:</span>
             <span className="value">{usageData.monthlyUsage.toFixed(2)} ç¾å…ƒ</span>
           </div>
           
           <div className="stat-item">
             <span className="label">é¢„ç®—é™åˆ¶:</span>
             <span className="value">{usageData.costLimit.toFixed(2)} ç¾å…ƒ</span>
           </div>
           
           <div className="stat-item">
             <span className="label">å‰©ä½™é¢„ç®—:</span>
             <span className="value">{usageData.remainingBudget.toFixed(2)} ç¾å…ƒ</span>
           </div>
         </div>
         
         <div className="usage-bar">
           <div 
             className="usage-fill" 
             style={{ width: `${Math.min(usagePercentage, 100)}%` }}
           ></div>
         </div>
         
         <div className="usage-percentage">
           {usagePercentage.toFixed(1)}% å·²ä½¿ç”¨
         </div>
         
         {usagePercentage > 80 && (
           <div className="usage-warning">
             âš ï¸ æ¥è¿‘é¢„ç®—é™åˆ¶ï¼Œè¯·æ³¨æ„æ§åˆ¶ä½¿ç”¨
           </div>
         )}
       </div>
     );
   };
   
   export default UsageMonitor;
   ```

### 10.9 æµ‹è¯•ä¸éƒ¨ç½²

1. **æœ¬åœ°æµ‹è¯•**ï¼š
   ```bash
   # è®¾ç½®æœ¬åœ°ç¯å¢ƒå˜é‡
   echo "OPENAI_API_KEY=your-test-key-here" > .env.local
   echo "ANTHROPIC_API_KEY=your-test-key-here" >> .env.local
   
   # ç¡®ä¿å®‰è£…äº†å¿…è¦çš„ä¾èµ–
   npm install
   
   # å¯åŠ¨å¼€å‘æœåŠ¡å™¨
   npm run dev
   ```

2. **éƒ¨ç½²åˆ° Vercel**ï¼š
   ```bash
   # éƒ¨ç½²åˆ° Vercel
   vercel --prod
   
   # åœ¨ Vercel æ§åˆ¶å°è®¾ç½®ç¯å¢ƒå˜é‡
   # OPENAI_API_KEY
   # ANTHROPIC_API_KEY
   ```

3. **æµ‹è¯• API ç«¯ç‚¹**ï¼š
   ```bash
   # æµ‹è¯• LLM API ç«¯ç‚¹
   curl -X POST https://your-app.vercel.app/api/llm-edge \
     -H "Content-Type: application/json" \
     -d '{
       "messages": [
         {"role": "user", "content": "Hello, how are you?"}
       ],
       "provider": "openai"
     }'
   ```

### 10.10 æ•…éšœæ’é™¤ä¸æœ€ä½³å®è·µ

1. **å¸¸è§é—®é¢˜**ï¼š
   - **é€Ÿç‡é™åˆ¶é”™è¯¯**ï¼šå¢åŠ è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿæˆ–å®æ–½æŒ‡æ•°é€€é¿
   - **ä»¤ç‰Œé™åˆ¶**ï¼šå‡å° `max_tokens` å‚æ•°æˆ–åˆ†å‰²é•¿æ–‡æœ¬
   - **æ¨¡å‹ä¸å¯ç”¨**ï¼šæ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®ï¼Œæˆ–å°è¯•å›é€€åˆ°å…¶ä»–æ¨¡å‹

2. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - å®ç°è¯·æ±‚ç¼“å­˜æœºåˆ¶
   - ä½¿ç”¨æµå¼å“åº”å¤„ç†é•¿æ–‡æœ¬
   - ä¼˜åŒ–æç¤ºè¯ä»¥å‡å°‘ä»¤ç‰Œä½¿ç”¨

3. **å®‰å…¨æœ€ä½³å®è·µ**ï¼š
   - å®šæœŸè½®æ¢ API å¯†é’¥
   - ç›‘æ§å¼‚å¸¸ä½¿ç”¨æ¨¡å¼
   - å®æ–½æœ€å°æƒé™åŸåˆ™
   - è®°å½•æ‰€æœ‰ API è°ƒç”¨ä»¥è¿›è¡Œå®¡è®¡

é€šè¿‡ä»¥ä¸Šé…ç½®ï¼Œæ‚¨çš„ Edge-TTS Web åº”ç”¨ç°åœ¨å¯ä»¥å®‰å…¨åœ°é›†æˆ LLM åŠŸèƒ½ï¼ŒåŒæ—¶ä¿æŠ¤ API å¯†é’¥ä¸è¢«æ³„éœ²ï¼Œå¹¶æä¾›äº†å…¨é¢çš„ä½¿ç”¨ç›‘æ§å’Œæˆæœ¬æ§åˆ¶æœºåˆ¶ã€‚